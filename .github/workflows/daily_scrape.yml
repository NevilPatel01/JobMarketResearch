# Daily job collection and processing

name: Daily Job Scraper

on:
  schedule:
    - cron: '0 7 * * *'  # 7 AM UTC daily (2 AM EST)
  workflow_dispatch:  # Allow manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      SUPABASE_DB_POOLER_URL: ${{ secrets.SUPABASE_DB_POOLER_URL }}
      SUPABASE_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
      ADZUNA_APP_ID: ${{ secrets.ADZUNA_APP_ID }}
      ADZUNA_APP_KEY: ${{ secrets.ADZUNA_APP_KEY }}
      RAPIDAPI_KEY: ${{ secrets.RAPIDAPI_KEY }}
      RAPIDAPI_HOST: ${{ secrets.RAPIDAPI_HOST }}
    steps:
      - uses: actions/checkout@v4

      - name: Verify secrets
        run: |
          if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_KEY" ]; then
            echo "ERROR: SUPABASE_URL or SUPABASE_KEY is empty. Check repo secrets."
            exit 1
          fi
          echo "Secrets OK (SUPABASE_URL and SUPABASE_KEY are set)"

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install streamlit plotly

      - name: Collect jobs
        run: |
          python collect_multi_source.py --sources jobbank adzuna remoteok --target 6000
        continue-on-error: true

      - name: Process and extract features
        run: |
          python run.py process
