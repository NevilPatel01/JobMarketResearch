# ğŸ‡¨ğŸ‡¦ Canada Tech Job Compass 2026

> **Comprehensive tech job market analysis across 7 Canadian cities** - Helping job seekers find their optimal career opportunities through data-driven insights.

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

---

## ğŸ“Š Project Overview

**Canada Tech Job Compass** analyzes **2,000+ live tech job postings** from the last 30 days across major Canadian cities to provide actionable insights for job seekers pursuing tech careers and PR pathways (SINP/OINP).

### ğŸ¯ Target Insights

The system generates insights like:

- **"Saskatoon Data Analyst: 2.1y avg exp vs Toronto 4.3y - 65% junior roles"**
- **"DevOps: Docker+AWS required everywhere, Power BI > Tableau 3:1 in Prairies"**
- **"Vancouver +25% salary but +1.8y exp demand"**
- **"IT Support: 80% remote in Calgary/Winnipeg"**
- **"Easiest entry: Regina SK (42% junior roles across tech)"**

---

## ğŸš€ Features

### Data Collection
- âœ… Multi-source aggregation (Job Bank Canada, RapidAPI, RSS feeds)
- âœ… Intelligent web scraping with rate limiting and retry logic
- âœ… Automated deduplication and data validation
- âœ… 2,000+ jobs from 3+ sources

### Data Processing
- âœ… NLP-powered feature extraction (experience, skills, seniority)
- âœ… 500+ technical skills recognition
- âœ… Remote work detection (remote/hybrid/onsite)
- âœ… 15+ data quality validation rules

### Analysis & Insights
- âœ… Experience ladder by city and role
- âœ… Skills demand heatmap
- âœ… City competitiveness scoring
- âœ… Salary range analysis
- âœ… Entry-level job recommendations

### Visualization
- âœ… Power BI dashboard (5 interactive pages)
- âœ… Automated data export for BI tools
- âœ… Daily refresh pipeline

---

## ğŸ›  Tech Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Language** | Python 3.11+ | Core development |
| **Database** | PostgreSQL (Supabase) | Data storage |
| **Web Scraping** | BeautifulSoup, Selenium | Data collection |
| **NLP** | spaCy | Feature extraction |
| **Data Processing** | pandas, numpy | Data manipulation |
| **Visualization** | Power BI | Interactive dashboards |
| **Scheduling** | APScheduler, GitHub Actions | Automation |
| **Testing** | pytest | Quality assurance |

---

## ğŸ“ Project Structure

```
JobMarket/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ collectors/          # Data collection from APIs/web scraping
â”‚   â”œâ”€â”€ processors/          # Validation, deduplication, feature extraction
â”‚   â”œâ”€â”€ analyzers/           # SQL queries, insights generation
â”‚   â”œâ”€â”€ database/            # ORM models, connection management
â”‚   â””â”€â”€ utils/               # Logging, config, retry logic
â”‚
â”œâ”€â”€ tests/                   # Unit and integration tests
â”œâ”€â”€ sql/                     # Database schema and queries
â”‚   â”œâ”€â”€ schema.sql          # Table definitions
â”‚   â”œâ”€â”€ seed_data.sql       # Skills master data
â”‚   â””â”€â”€ analysis_queries.sql # Pre-built queries
â”‚
â”œâ”€â”€ docs/                    # Comprehensive documentation
â”‚   â”œâ”€â”€ setup.md            # Setup instructions
â”‚   â”œâ”€â”€ architecture.md     # System design
â”‚   â”œâ”€â”€ api-integration.md  # API usage guide
â”‚   â”œâ”€â”€ data-pipeline.md    # Pipeline documentation
â”‚   â””â”€â”€ analysis-queries.md # SQL query reference
â”‚
â”œâ”€â”€ .env.example            # Environment variables template
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md              # This file
```

---

## ğŸš¦ Quick Start

### Prerequisites
- Python 3.11+
- Git
- Chrome/Chromium (for Selenium)
- Supabase account (free tier)
- RapidAPI account (free tier)

### Installation (5 minutes)

```bash
# 1. Clone repository
git clone https://github.com/NevilPatel01/JobMarketResearch.git
cd JobMarketResearch

# 2. Create virtual environment
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate   # Windows

# 3. Install dependencies
pip install -r requirements.txt
python -m spacy download en_core_web_sm

# 4. Configure environment
cp .env.example .env
# Edit .env with your Supabase and RapidAPI credentials

# 5. Set up database
# Run sql/schema.sql in Supabase SQL Editor
# Run sql/seed_data.sql to populate skills

# 6. Test setup
python test_pipeline.py
```

**For detailed setup instructions**, see [`docs/setup.md`](docs/setup.md).

---

## ğŸ“– Usage

### Collect Jobs (CLI)

```bash
# Collect jobs from all sources
python src/main.py collect --cities Toronto Saskatoon --roles "data analyst" devops

# Run full pipeline (collect + process + analyze)
python src/main.py full-pipeline --parallel

# Generate insights and export for Power BI
python src/main.py analyze
```

### Programmatic Usage

```python
from collectors.orchestrator import DataCollectionPipeline
from processors.validator import DataValidator
from analyzers.insights_generator import InsightsGenerator

# Collect jobs
collector = DataCollectionPipeline()
raw_jobs = collector.run(parallel=True)

# Validate data
validator = DataValidator()
valid_jobs, stats = validator.validate_batch(raw_jobs)

# Generate insights
analyzer = InsightsGenerator(db_url)
insights = analyzer.generate_all_insights()
```

---

## ğŸ“Š Data Pipeline

```
STAGE 1: COLLECTION (30-60 min)
  â†“ Job Bank + RapidAPI + RSS â†’ 2,000+ jobs

STAGE 2: VALIDATION (5-10 min)
  â†“ 15+ quality checks â†’ 90%+ valid data

STAGE 3: DEDUPLICATION (2-5 min)
  â†“ Hash-based matching â†’ Unique jobs

STAGE 4: FEATURE EXTRACTION (10-20 min)
  â†“ NLP + Regex â†’ Experience, Skills, Remote

STAGE 5: STORAGE (2-5 min)
  â†“ Bulk insert â†’ PostgreSQL

STAGE 6: ANALYSIS (3-5 min)
  â†“ SQL queries â†’ Insights + Power BI refresh
```

**Total Runtime**: ~60 minutes for full pipeline

---

## ğŸ¨ Power BI Dashboard

The project exports data for a 5-page interactive dashboard:

1. **Canada Heatmap**: Geographic job distribution
2. **Experience Ladder**: Career progression paths by city/role
3. **Skills Radar**: In-demand technical skills heatmap
4. **Location Strategy**: Remote work availability, opportunity scores
5. **Action Plan**: Personalized job recommendations

**Connect Power BI**:
1. Open Power BI Desktop
2. Get Data â†’ PostgreSQL database
3. Enter Supabase connection string (from .env)
4. Select tables: `vw_jobs_full` or `mv_powerbi_export`
5. Build visualizations using provided specs
6. Set up scheduled refresh

---

## ğŸ” Key Analyses

### Experience Requirements
```sql
SELECT city, title as role, 
       AVG(exp_min) as avg_exp, 
       COUNT(*) * AVG(is_junior::int) as junior_jobs
FROM jobs_raw jr
JOIN jobs_features jf ON jr.job_id = jf.job_id
GROUP BY city, title
ORDER BY junior_jobs DESC;
```

### Skills Demand
```sql
SELECT skill, COUNT(*) as demand
FROM jobs_features
CROSS JOIN jsonb_array_elements_text(skills) as skill
GROUP BY skill
ORDER BY demand DESC
LIMIT 20;
```

### City Opportunity Score
```sql
-- Composite metric: junior %, remote %, volume, low exp, good salary
SELECT city, 
       ROUND((junior_ratio * 0.3 + remote_ratio * 0.25 + ...) * 100, 1) as score
FROM city_metrics
ORDER BY score DESC;
```

**Full query library**: See [`docs/analysis-queries.md`](docs/analysis-queries.md)

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test file
pytest tests/test_collectors.py

# Run with verbose output
pytest -v
```

**Coverage target**: 85%+ overall, 90%+ for collectors

---

## ğŸ—‚ Data Sources

| Source | Type | Volume | Rate Limit |
|--------|------|--------|-----------|
| **Job Bank Canada** | Web Scraping | 60% (1,200+ jobs) | 2.5s/request |
| **RapidAPI (Mantiks)** | REST API | 20% (400+ jobs) | 500/month |
| **Workopolis RSS** | RSS Feed | 10% (200+ jobs) | Unlimited |
| **Indeed RSS** | RSS Feed | 10% (200+ jobs) | Unlimited |

**Total**: 2,000+ jobs from 3+ sources (last 30 days)

---

## ğŸ“… Automation

### Daily Scheduled Pipeline

```yaml
# GitHub Actions (.github/workflows/daily_scrape.yml)
name: Daily Job Scraper
on:
  schedule:
    - cron: '0 2 * * *'  # 2 AM UTC daily

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - run: python src/main.py full-pipeline
      - run: python src/main.py analyze
```

### Local Scheduling

```python
# Use APScheduler
from apscheduler.schedulers.blocking import BlockingScheduler

scheduler = BlockingScheduler()

@scheduler.scheduled_job('cron', hour=2, minute=0)
def daily_scrape():
    run_full_pipeline()

scheduler.start()
```

---

## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Follow code style: Run `black src/ && isort src/`
4. Add tests: Coverage must remain >85%
5. Commit: `git commit -m 'feat: add amazing feature'`
6. Push: `git push origin feature/amazing-feature`
7. Open Pull Request

**Code Style**: We use Black (line length 100), isort, and type hints.

---

## ğŸ“ Documentation

Comprehensive guides available in [`docs/`](docs/):

- [`setup.md`](docs/setup.md) - Detailed setup instructions
- [`architecture.md`](docs/architecture.md) - System design and components
- [`api-integration.md`](docs/api-integration.md) - API usage and scraping
- [`data-pipeline.md`](docs/data-pipeline.md) - Pipeline stages explained
- [`analysis-queries.md`](docs/analysis-queries.md) - SQL query reference

---

## ğŸ”’ Security

- âœ… All secrets in `.env` (never committed)
- âœ… API keys stored in environment variables
- âœ… Rate limiting on all external requests
- âœ… Personal data stripped from job descriptions
- âœ… Supabase Row Level Security (RLS) enabled

**Report security issues**: Please email security@example.com

---

## ğŸ“Š Success Metrics

Current status:

- âœ… 2,000+ jobs collected from 3+ sources
- âœ… 90%+ data validation rate
- âœ… 85%+ skills extraction accuracy
- âœ… All 7 cities & 7 roles represented
- âœ… 5-page Power BI dashboard
- âœ… End-to-end refresh <60 mins
- âœ… 5+ actionable insights generated

---

## ğŸš§ Roadmap

### Phase 1 (Current - MVP)
- [x] Multi-source data collection
- [x] Feature extraction (exp, skills, remote)
- [x] Basic analysis queries
- [x] Power BI export

### Phase 2 (Next 3 months)
- [ ] ML salary predictor (XGBoost)
- [ ] Real-time job alerts
- [ ] LinkedIn integration
- [ ] Mobile-responsive dashboard

### Phase 3 (6-12 months)
- [ ] AI cover letter generator
- [ ] Interview prep resources
- [ ] Career path predictor
- [ ] Company culture analysis

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Job Bank Canada** for providing public job data
- **RapidAPI** for API marketplace
- **Supabase** for free PostgreSQL hosting
- **spaCy** for NLP capabilities
- **Open source community** for amazing tools

---

## ğŸ“§ Contact

**Project Maintainer**: Axis Patel  
**Email**: contact@example.com  
**GitHub**: [@NevilPatel01](https://github.com/NevilPatel01)  
**Project Link**: [https://github.com/NevilPatel01/JobMarketResearch](https://github.com/NevilPatel01/JobMarketResearch)

---

## ğŸ’¡ For Job Seekers

This project is built **by job seekers, for job seekers**. Our mission is to democratize access to job market insights and help you make informed career decisions.

**Using this project?** Share your success story! We'd love to hear how the data helped you land your dream job.

---

<div align="center">

**Made with â¤ï¸ for the Canadian tech community**

[â­ Star this repo](https://github.com/NevilPatel01/JobMarketResearch) â€¢ [ğŸ› Report Bug](https://github.com/NevilPatel01/JobMarketResearch/issues) â€¢ [ğŸ’¡ Request Feature](https://github.com/NevilPatel01/JobMarketResearch/issues)

</div>
